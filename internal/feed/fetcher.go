package feed

import (
	"MrRSS/internal/database"
	"MrRSS/internal/models"
	"MrRSS/internal/rules"
	"MrRSS/internal/translation"
	"MrRSS/internal/utils"
	"context"
	"log"
	"net/http"
	"strconv"
	"sync"
	"time"

	"github.com/mmcdole/gofeed"
)

// FeedParser interface to allow mocking
type FeedParser interface {
	ParseURL(url string) (*gofeed.Feed, error)
	ParseURLWithContext(url string, ctx context.Context) (*gofeed.Feed, error)
}

type Fetcher struct {
	db                *database.DB
	fp                FeedParser
	highPriorityFp    FeedParser // High priority parser for content fetching
	translator        translation.Translator
	scriptExecutor    *ScriptExecutor
	progress          Progress
	mu                sync.Mutex
	refreshCalculator *IntelligentRefreshCalculator
	taskManager       *TaskManager
	cleanupManager    *CleanupManager
}

func NewFetcher(db *database.DB, translator translation.Translator) *Fetcher {
	// Initialize script executor with scripts directory
	scriptsDir, err := utils.GetScriptsDir()
	var executor *ScriptExecutor
	if err == nil {
		executor = NewScriptExecutor(scriptsDir)
	}

	// Create HTTP client for feed parsing
	httpClient, err := CreateHTTPClient("")
	if err != nil {
		// Fallback to default client if proxy setup fails
		httpClient = &http.Client{Timeout: 30 * time.Second}
	}

	// Create parser with custom HTTP client to support localhost and other endpoints
	parser := gofeed.NewParser()
	parser.Client = httpClient

	// Create high priority parser with shorter timeout for content fetching
	highPriorityParser := gofeed.NewParser()
	highPriorityParser.Client = httpClient

	fetcher := &Fetcher{
		db:                db,
		fp:                parser,
		highPriorityFp:    highPriorityParser,
		translator:        translator,
		scriptExecutor:    executor,
		refreshCalculator: NewIntelligentRefreshCalculator(db),
	}

	// Initialize task manager with default capacity
	fetcher.taskManager = NewTaskManager(fetcher, 5)
	fetcher.taskManager.Start()

	// Initialize cleanup manager
	fetcher.cleanupManager = NewCleanupManager(fetcher)
	fetcher.cleanupManager.Start()

	return fetcher
}

// GetIntelligentRefreshCalculator returns the refresh calculator
func (f *Fetcher) GetIntelligentRefreshCalculator() *IntelligentRefreshCalculator {
	return f.refreshCalculator
}

// GetStaggeredDelay calculates a staggered delay for feed refresh
func (f *Fetcher) GetStaggeredDelay(feedID int64, totalFeeds int) time.Duration {
	return GetStaggeredDelay(feedID, totalFeeds)
}

// GetTaskManager returns the task manager
func (f *Fetcher) GetTaskManager() *TaskManager {
	return f.taskManager
}

// GetCleanupManager returns the cleanup manager
func (f *Fetcher) GetCleanupManager() *CleanupManager {
	return f.cleanupManager
}

// getDataDir returns the data directory path
func (f *Fetcher) getDataDir() (string, error) {
	return utils.GetDataDir()
}

// getConcurrencyLimit returns the maximum number of concurrent feed refreshes
// based on network detection or defaults to 5 if not configured
func (f *Fetcher) getConcurrencyLimit(feedCount int) int {
	concurrencyStr, err := f.db.GetSetting("max_concurrent_refreshes")
	if err != nil || concurrencyStr == "" {
		return 5 // Default concurrency if network detection failed or not run
	}

	concurrency, err := strconv.Atoi(concurrencyStr)
	if err != nil || concurrency < 1 {
		return 5 // Default on parse error or invalid value
	}

	// Cap at reasonable limits (increased from 20 to 30 for faster networks)
	if concurrency > 30 {
		concurrency = 30
	}

	return concurrency
}

// getHTTPClient returns an HTTP client configured with proxy if needed
// Proxy precedence (highest to lowest):
// 1. Feed custom proxy (ProxyEnabled=true, ProxyURL != "")
// 2. Global proxy (ProxyEnabled=true, ProxyURL == "", global proxy_enabled=true)
// 3. No proxy (ProxyEnabled=false or no global proxy)
func (f *Fetcher) getHTTPClient(feed models.Feed) (*http.Client, error) {
	var proxyURL string

	// Check feed-level proxy settings
	if feed.ProxyEnabled && feed.ProxyURL != "" {
		// Feed has custom proxy configured - highest priority
		proxyURL = feed.ProxyURL
	} else if feed.ProxyEnabled {
		// Feed requests to use global proxy
		proxyEnabled, _ := f.db.GetSetting("proxy_enabled")
		if proxyEnabled == "true" {
			// Build global proxy URL from settings (use encrypted methods for credentials)
			proxyType, _ := f.db.GetSetting("proxy_type")
			proxyHost, _ := f.db.GetSetting("proxy_host")
			proxyPort, _ := f.db.GetSetting("proxy_port")
			proxyUsername, _ := f.db.GetEncryptedSetting("proxy_username")
			proxyPassword, _ := f.db.GetEncryptedSetting("proxy_password")
			proxyURL = BuildProxyURL(proxyType, proxyHost, proxyPort, proxyUsername, proxyPassword)
		}
	}
	// If ProxyEnabled=false, proxyURL remains empty (no proxy)

	// Create HTTP client with or without proxy
	return CreateHTTPClient(proxyURL)
}

// setupTranslator configures the translator based on database settings.
// Now supports global proxy settings for all translation services.
func (f *Fetcher) setupTranslator() {
	provider, _ := f.db.GetSetting("translation_provider")

	var t translation.Translator
	switch provider {
	case "deepl":
		apiKey, _ := f.db.GetEncryptedSetting("deepl_api_key")
		if apiKey != "" {
			t = translation.NewDeepLTranslatorWithDB(apiKey, f.db)
		} else {
			t = translation.NewGoogleFreeTranslatorWithDB(f.db)
		}
	case "baidu":
		appID, _ := f.db.GetSetting("baidu_app_id")
		secretKey, _ := f.db.GetEncryptedSetting("baidu_secret_key")
		if appID != "" && secretKey != "" {
			t = translation.NewBaiduTranslatorWithDB(appID, secretKey, f.db)
		} else {
			t = translation.NewGoogleFreeTranslatorWithDB(f.db)
		}
	case "ai":
		apiKey, _ := f.db.GetEncryptedSetting("ai_api_key")
		endpoint, _ := f.db.GetSetting("ai_endpoint")
		model, _ := f.db.GetSetting("ai_model")
		if apiKey != "" {
			t = translation.NewAITranslatorWithDB(apiKey, endpoint, model, f.db)
			// Set custom headers if available
			if aiTranslator, ok := t.(*translation.AITranslator); ok {
				customHeaders, _ := f.db.GetSetting("ai_custom_headers")
				aiTranslator.SetCustomHeaders(customHeaders)
			}
		} else {
			t = translation.NewGoogleFreeTranslatorWithDB(f.db)
		}
	default:
		// Default to Google Free Translator with proxy support
		t = translation.NewGoogleFreeTranslatorWithDB(f.db)
	}
	f.translator = t
}

func (f *Fetcher) FetchAll(ctx context.Context) {
	// Get all feeds
	feeds, err := f.db.GetFeeds()
	if err != nil {
		log.Println("Error getting feeds:", err)
		return
	}

	if len(feeds) == 0 {
		log.Println("No feeds to refresh")
		// Mark progress as completed since there's nothing to do
		f.taskManager.MarkCompleted()
		return
	}

	// Update task manager capacity based on network
	concurrency := f.getConcurrencyLimit(len(feeds))
	f.taskManager.SetPoolCapacity(concurrency)

	// Use task manager for global refresh (all feeds go to queue tail)
	f.taskManager.AddGlobalRefresh(ctx, feeds)
}

func (f *Fetcher) FetchFeed(ctx context.Context, feed models.Feed) {
	// Use ParseFeedWithFeed with normal priority for feed refresh
	parsedFeed, err := f.ParseFeedWithFeed(ctx, &feed, false) // Normal priority for refresh
	if err != nil {
		log.Printf("Error parsing feed %s: %v", feed.URL, err)
		f.db.UpdateFeedError(feed.ID, err.Error())
		// Add error to progress for immediate feedback
		f.mu.Lock()
		if f.progress.Errors == nil {
			f.progress.Errors = make(map[int64]string)
		}
		f.progress.Errors[feed.ID] = err.Error()
		f.mu.Unlock()
		return
	}

	// Clear any previous error on successful fetch
	f.db.UpdateFeedError(feed.ID, "")

	// Update Feed Image if available and not set
	if feed.ImageURL == "" && parsedFeed.Image != nil {
		f.db.UpdateFeedImage(feed.ID, parsedFeed.Image.URL)
	}

	// Update Feed Link if available and not set
	if feed.Link == "" && parsedFeed.Link != "" {
		f.db.UpdateFeedLink(feed.ID, parsedFeed.Link)
	}

	// Process articles
	articlesWithContent := f.processArticles(feed, parsedFeed.Items)

	// Check context before heavy DB operation
	select {
	case <-ctx.Done():
		return
	default:
	}

	if len(articlesWithContent) > 0 {
		// Extract just the articles for saving
		articlesToSave := make([]*models.Article, len(articlesWithContent))
		for i, awc := range articlesWithContent {
			articlesToSave[i] = awc.Article
		}

		if err := f.db.SaveArticles(ctx, articlesToSave); err != nil {
			log.Printf("Error saving articles for feed %s: %v", feed.Title, err)
		} else {
			// Cache article content from RSS feed
			f.cacheArticleContents(articlesWithContent)

			// Apply rules to newly saved articles
			// We fetch the recent articles for this feed since SaveArticles doesn't return IDs
			// This is limited to the number of articles we just saved
			savedArticles, err := f.db.GetArticles("", feed.ID, "", false, len(articlesToSave), 0)
			if err == nil && len(savedArticles) > 0 {
				engine := rules.NewEngine(f.db)
				affected, err := engine.ApplyRulesToArticles(savedArticles)
				if err != nil {
					log.Printf("Error applying rules for feed %s: %v", feed.Title, err)
				} else if affected > 0 {
					utils.DebugLog("Applied rules to %d articles in feed %s", affected, feed.Title)
				}
			}
		}
	}
	utils.DebugLog("Updated feed: %s", feed.Title)
}

// fetchFeedWithContext is the internal fetch method used by TaskManager
// Returns error instead of storing in progress.Errors
func (f *Fetcher) fetchFeedWithContext(ctx context.Context, feed models.Feed) error {
	// Use ParseFeedWithFeed with normal priority for feed refresh
	parsedFeed, err := f.ParseFeedWithFeed(ctx, &feed, false)
	if err != nil {
		return err
	}

	// Check context after parsing
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	// Clear any previous error on successful fetch
	f.db.UpdateFeedError(feed.ID, "")

	// Update Feed Image if available and not set
	if feed.ImageURL == "" && parsedFeed.Image != nil {
		f.db.UpdateFeedImage(feed.ID, parsedFeed.Image.URL)
	}

	// Update Feed Link if available and not set
	if feed.Link == "" && parsedFeed.Link != "" {
		f.db.UpdateFeedLink(feed.ID, parsedFeed.Link)
	}

	// Check context before processing articles
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	// Process articles
	articlesWithContent := f.processArticles(feed, parsedFeed.Items)

	// Check context before heavy DB operation
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	if len(articlesWithContent) > 0 {
		// Extract just the articles for saving
		articlesToSave := make([]*models.Article, len(articlesWithContent))
		for i, awc := range articlesWithContent {
			articlesToSave[i] = awc.Article
		}

		if err := f.db.SaveArticles(ctx, articlesToSave); err != nil {
			return err
		}

		// Post-processing operations (content caching and rule application)
		// These are non-critical and run asynchronously to avoid blocking the feed refresh
		// Even if they fail or are slow, the feed has already been successfully saved
		go func() {
			// Cache article content from RSS feed
			f.cacheArticleContents(articlesWithContent)

			// Apply rules to newly saved articles
			savedArticles, err := f.db.GetArticles("", feed.ID, "", false, len(articlesToSave), 0)
			if err != nil {
				log.Printf("Error getting articles for rule application: %v", err)
				return
			}
			if len(savedArticles) == 0 {
				return
			}

			engine := rules.NewEngine(f.db)
			affected, err := engine.ApplyRulesToArticles(savedArticles)
			if err != nil {
				log.Printf("Error applying rules for feed %s: %v", feed.Title, err)
			} else if affected > 0 {
				utils.DebugLog("Applied rules to %d articles in feed %s", affected, feed.Title)
			}
		}()
	}

	utils.DebugLog("Updated feed: %s", feed.Title)
	return nil
}

// FetchSingleFeed fetches a single feed with progress tracking.
// This is used when adding a new feed, refreshing a single feed from the context menu,
// or when the scheduler triggers individual feed refreshes.
// For manual operations (add/edit/refresh), place at queue head.
// For scheduled operations, place at queue tail.
func (f *Fetcher) FetchSingleFeed(ctx context.Context, feed models.Feed, isManual bool) {
	if isManual {
		// Manual operations go to queue head
		f.taskManager.AddToQueueHead(ctx, feed, TaskReasonManualRefresh)
	} else {
		// Scheduled operations go to queue tail
		f.taskManager.AddToQueueTail(ctx, feed, TaskReasonScheduledCustom)
	}
}

// FetchFeedForArticle fetches a feed immediately when article content is missing.
// This bypasses the queue and pool limits.
func (f *Fetcher) FetchFeedForArticle(ctx context.Context, feed models.Feed) {
	f.taskManager.ExecuteImmediately(ctx, feed)
}

// FetchFeedsByIDs fetches multiple feeds by their IDs with progress tracking.
// This is used after OPML import or when editing feeds.
// All feeds are added to queue head (high priority).
func (f *Fetcher) FetchFeedsByIDs(ctx context.Context, feedIDs []int64) {
	if len(feedIDs) == 0 {
		return
	}

	// Fetch feeds by IDs
	for _, feedID := range feedIDs {
		feed, err := f.db.GetFeedByID(feedID)
		if err != nil {
			log.Printf("Error getting feed %d: %v", feedID, err)
			continue
		}
		// Add to queue head as high priority (manual add/edit)
		f.taskManager.AddToQueueHead(ctx, *feed, TaskReasonManualAdd)
	}
}

// cacheArticleContents caches article contents from RSS feeds
// This is called after articles are saved to the database
func (f *Fetcher) cacheArticleContents(articlesWithContent []*ArticleWithContent) {
	for _, awc := range articlesWithContent {
		// Only cache if content is not empty and URL is present
		if awc.Content == "" || awc.Article.URL == "" {
			continue
		}

		// Get article ID by unique_id (article was just saved, so it should exist)
		articleID, err := f.db.GetArticleIDByUniqueID(awc.Article.Title, awc.Article.FeedID, awc.Article.PublishedAt, awc.Article.HasValidPublishedTime)
		if err != nil {
			// Article might not exist yet (race condition) or other error
			utils.DebugLog("Could not find article ID for %s: %v", awc.Article.Title, err)
			continue
		}

		// Cache the content (this will overwrite any existing cache as required)
		if err := f.db.SetArticleContent(articleID, awc.Content); err != nil {
			log.Printf("Error caching content for article %d: %v", articleID, err)
		} else {
			utils.DebugLog("Cached content for article %d", articleID)
		}
	}
}
